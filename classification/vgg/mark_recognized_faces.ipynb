{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d42025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from retinaface import RetinaFace\n",
    "from tensorflow import keras\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ddc557",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90308a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.utils import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f58e770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.path.abspath(''))\n",
    "sys.path.append(str(cwd.parent.parent))\n",
    "sys.path.append(str(cwd.parent.parent/\"recognition\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55f7ec9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/kmielcar/Documents/projects/faces_classification/classification/vgg',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/kmielcar/Documents/projects/faces_classification/venv/lib/python3.8/site-packages',\n",
       " PosixPath('/Users/kmielcar/Documents/projects/faces_classification/classification'),\n",
       " PosixPath('/Users/kmielcar/Documents/projects/faces_classification'),\n",
       " '/Users/kmielcar/Documents/projects/faces_classification',\n",
       " '/Users/kmielcar/Documents/projects/faces_classification',\n",
       " '/Users/kmielcar/Documents/projects/faces_classification',\n",
       " '/Users/kmielcar/Documents/projects/faces_classification',\n",
       " '/Users/kmielcar/Documents/projects/faces_classification/recognition']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c4a8fa8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crop import crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7c9351e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d601190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bfc9c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'common' from '/Users/kmielcar/Documents/projects/faces_classification/common.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75e79ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import EXPECTED_SIZE, GIRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aee5027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = Path(\"../../images/test/Grafiti/\")\n",
    "output_dir = Path(\"../../images/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6e6639",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_paths = list(test_dir.glob(\"*jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24456738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('trained_vgg_on_girls/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c1a7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_path:Path):\n",
    "    detected_faces_desc = RetinaFace.detect_faces(str(img_path))\n",
    "    faces = detected_faces_desc.values()\n",
    "    if not faces:\n",
    "        return\n",
    "\n",
    "    img_bgr = cv2.imread(str(img_path))\n",
    "    \n",
    "    for face_details in faces:\n",
    "        face_roi = face_details['facial_area']\n",
    "\n",
    "        cropped_face_img = crop(img_bgr, *face_roi)\n",
    "        \n",
    "        cropped_face_img = cropped_face_img.astype('float32')\n",
    "        sample = np.expand_dims(cropped_face_img, axis=0)\n",
    "        sample = preprocess_input(sample, version=2)\n",
    "        \n",
    "        pred = model.predict(sample)\n",
    "        max_val_ind = np.argmax(pred[0])\n",
    "        proba = pred[0][max_val_ind]\n",
    "        \n",
    "        text = \"{}: {:.2f}%\".format(GIRLS[max_val_ind], proba * 100)\n",
    "        left, top, right, bottom = face_roi\n",
    "        y = top - 10 if top - 10 > 10 else top + 10\n",
    "        cv2.rectangle(img_bgr, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        marked_img = cv2.putText(img_bgr, text, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        \n",
    "        out_path = output_dir / img_path.name\n",
    "        cv2.imwrite(str(out_path), marked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac91b637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1657797459961.jpg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88996696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mprocess_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36mprocess_img\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_details \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[1;32m     10\u001b[0m     face_roi \u001b[38;5;241m=\u001b[39m face_details[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacial_area\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     cropped_face_img \u001b[38;5;241m=\u001b[39m \u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mface_roi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     cropped_face_img \u001b[38;5;241m=\u001b[39m cropped_face_img\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(cropped_face_img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/projects/faces_classification/recognition/crop.py:12\u001b[0m, in \u001b[0;36mcrop\u001b[0;34m(path_or_array, left, top, right, bottom)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop\u001b[39m(path_or_array, left, top, right, bottom):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCustomCrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottom\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/faces_classification/recognition/crop.py:55\u001b[0m, in \u001b[0;36mCustomCrop.crop\u001b[0;34m(self, path_or_array, left, top, right, bottom)\u001b[0m\n\u001b[1;32m     52\u001b[0m image \u001b[38;5;241m=\u001b[39m image[pos[\u001b[38;5;241m0\u001b[39m]: pos[\u001b[38;5;241m1\u001b[39m], pos[\u001b[38;5;241m2\u001b[39m]: pos[\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Resize\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_AREA\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Underexposition\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma:\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "for img_path in image_paths:\n",
    "    process_img(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29fb0051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1525, 1936, 1782, 2288]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb028d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = cv2.imread(str(test_img))\n",
    "img_color = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "280e787c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detected_faces_desc = RetinaFace.detect_faces(str(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05a29432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "for face_details in detected_faces_desc.values():\n",
    "    face_roi = face_details['facial_area']\n",
    "    \n",
    "    cropped_face_img = crop(img_bgr, *face_roi)\n",
    "    cropped_face_img = cropped_face_img.astype('float32')\n",
    "    sample = np.expand_dims(cropped_face_img, axis=0)\n",
    "    sample = preprocess_input(sample, version=2)\n",
    "    pred = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74113889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7874803e-22, 1.0000000e+00, 1.6545585e-19]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c58ccc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a25b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = pred[0][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b0cf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{}: {:.2f}%\".format(GIRLS[ind], proba * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cb640e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, top, right, bottom = face_roi\n",
    "y = top - 10 if top - 10 > 10 else top + 10\n",
    "cv2.rectangle(img_bgr, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "marked_img = cv2.putText(img_bgr, text, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2094e7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"out.jpg\", marked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c9fc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "process_img(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c2102ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    }
   ],
   "source": [
    "process_img(test_dir/\"1657797460255.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
